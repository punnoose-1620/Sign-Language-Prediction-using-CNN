{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 619,
     "status": "ok",
     "timestamp": 1652039331598,
     "user": {
      "displayName": "Punnoose K Thomas",
      "userId": "12830390031589814980"
     },
     "user_tz": -330
    },
    "id": "KVHHGliuNi1D"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "#Creating the dimensions for the ROI...\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "\n",
    "\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Bx0bvf5LZSR"
   },
   "source": [
    "# Load the data using ImageDataGenerator of keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jY6Ct88BLJ5A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2710 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = r'D:\\Documents\\Ophir Projects\\Sign Language Prediction\\Dataset\\train'\n",
    "test_path = r'D:\\Documents\\Ophir Projects\\Sign Language Prediction\\Dataset\\test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFC2WfEaLdSY"
   },
   "source": [
    "plotImages function is for plotting images of the dataset loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1652039378881,
     "user": {
      "displayName": "Punnoose K Thomas",
      "userId": "12830390031589814980"
     },
     "user_tz": -330
    },
    "id": "FmjmMn9rLeSI",
    "outputId": "8bd0d022-5cff-48f7-a42d-d02c2aa0b444"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imgs, labels = next(train_batches)\n",
    "\n",
    "#Plotting the images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGss8ZJQLnJq"
   },
   "source": [
    "# Design the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e52i-Ol8LfJu"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(10,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttoev-04LvwY"
   },
   "source": [
    "# Fit the model and save the model for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVVofd3lL_xd"
   },
   "source": [
    "Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_6KQ8hqRLfGL"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4cq22V8L7U4"
   },
   "source": [
    "After compiling the model we fit the model on the train batches for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gaFxfUUQLfDY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "271/271 [==============================] - 13s 38ms/step - loss: 0.6375 - accuracy: 0.9491 - val_loss: 0.0504 - val_accuracy: 0.9933 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "271/271 [==============================] - 11s 42ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "271/271 [==============================] - 11s 40ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "271/271 [==============================] - 10s 38ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 8.0634e-04 - accuracy: 1.0000 - val_loss: 0.0173 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "271/271 [==============================] - 10s 35ms/step - loss: 6.2118e-04 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 5.0455e-04 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 4.2179e-04 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "271/271 [==============================] - 10s 35ms/step - loss: 3.6010e-04 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "271/271 [==============================] - 9s 35ms/step - loss: 3.1482e-04 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkbBJ4qGMXLC"
   },
   "source": [
    "Evaluating the model on the test set and printing the accuracy and loss scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zOgj-FYBLfA4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 0.006635932717472315; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "# For getting next batch of testing imgs...\n",
    "imgs, labels = next(test_batches) \n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "#Once the model is fitted we save the model using model.save()  function.\n",
    "\n",
    "model.save('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuEeQXOiMnjF"
   },
   "source": [
    "Visualizing and making a small test on the model to check if everything is working as we expect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MWsv-kw5Le-Z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions on a small set of test data--\n",
      "\n",
      "Man   Welcome   Woman   Happy   Happy   Afternoon   Morning   Morning   Morning   Hello   "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels\n",
      "Man   Welcome   Woman   Happy   Happy   Afternoon   Morning   Morning   Morning   Hello   "
     ]
    }
   ],
   "source": [
    "word_dict = {0:'Afternoon',1:'Good',2:'Happy',3:'Hello',4:'Man',5:'Morning',6:'Night',7:'Thank You',8:'Welcome',9:'Woman'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGL_kZr8MvVD"
   },
   "source": [
    "# Predict the gesture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62WSLV4hMz1D"
   },
   "source": [
    "Getting the necessary imports for model_for_gesture.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K1I1wzo0Le76"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKFFHUbIM5Oq"
   },
   "source": [
    " Load the model that we had created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rUKKsLl8LeyV"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(r\"best_model.h5\")\n",
    "\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7FNR3X5NIJV"
   },
   "source": [
    "Function to calculate the background accumulated weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-wnu4pJgNH4b"
   },
   "outputs": [],
   "source": [
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btZGjkSDNNQ7"
   },
   "source": [
    "Segmenting the hand, i.e, getting the max contours and the thresholded image of the hand detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZsflL3BQNH1A"
   },
   "outputs": [],
   "source": [
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "    cv2.imshow(\"Diff\",diff)\n",
    "    \n",
    "    print(\"testVal: \",cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY))\n",
    "    \n",
    "    val1 , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    cv2.imshow(\"Thresholded: \",thresholded)\n",
    "    print(\"Param1: \",cv2.RETR_EXTERNAL)\n",
    "    print(\"Param2: \",cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Fetching contours in the frame (These contours can be of hand or any other object in foreground) …\n",
    "    \n",
    "    image, contours, hierarchy = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If length of contours list = 0, means we didn't get any\n",
    "    #contours...\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # The largest external contour should be the hand \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Returning the hand segment(max contour) and the\n",
    "        # thresholded image of hand...\n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rd6E6cb8NSOd"
   },
   "source": [
    "Detecting the hand now on the live cam feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yQaNEleiNHx5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff:  [[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [2 2 2 ... 3 3 3]\n",
      " [2 2 2 ... 3 3 4]\n",
      " [2 2 2 ... 3 3 4]]\n",
      "testVal:  (2.0, array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  0,   0,   0, ..., 255, 255, 255],\n",
      "       [  0,   0,   0, ..., 255, 255, 255],\n",
      "       [  0,   0,   0, ..., 255, 255, 255]], dtype=uint8))\n",
      "Thresholded:  [[  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 255 255 255]\n",
      " [  0   0   0 ... 255 255 255]\n",
      " [  0   0   0 ... 255 255 255]]\n",
      "Param1:  0\n",
      "Param2:  2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PUNNOO~1\\AppData\\Local\\Temp/ipykernel_13656/3657793842.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# segmenting the hand region\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mhand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment_hand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray_frame\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Checking if we are able to detect the hand...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PUNNOO~1\\AppData\\Local\\Temp/ipykernel_13656/3681016972.py\u001b[0m in \u001b[0;36msegment_hand\u001b[1;34m(frame, threshold)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Fetching contours in the frame (These contours can be of hand or any other object in foreground) …\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontours\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhierarchy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresholded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETR_EXTERNAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCHAIN_APPROX_SIMPLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# If length of contours list = 0, means we didn't get any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "num_frames =0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # flipping the frame to prevent inverted image of captured frame...\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # ROI from the frame\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "\n",
    "    if num_frames < 70:\n",
    "        \n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "    \n",
    "    else: \n",
    "        # segmenting the hand region\n",
    "        hand = segment_hand(gray_frame,2)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.imshow(\"Thesholded Hand Image\", thresholded)\n",
    "            \n",
    "            thresholded = cv2.resize(thresholded, (64, 64))\n",
    "            thresholded = cv2.cvtColor(thresholded, cv2.COLOR_GRAY2RGB)\n",
    "            thresholded = np.reshape(thresholded, (1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "            \n",
    "            pred = model.predict(thresholded)\n",
    "            cv2.putText(frame_copy, word_dict[np.argmax(pred)], (170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "    # Draw ROI on frame_copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,\n",
    "    ROI_bottom), (255,128,0), 3)\n",
    "\n",
    "    # incrementing the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.putText(frame_copy, \"Sign Language Prediction Project\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "\n",
    "    # Close windows with Esc\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the camera and destroy all the windows\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNGERPakiu+w/tqjS75Wnl9",
   "collapsed_sections": [],
   "mount_file_id": "1Cs0LRDU3xOlh7kZplajDhgTPCsJmZ_Bk",
   "name": "PredictionModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
